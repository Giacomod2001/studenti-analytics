# Project Configuration
PROJECT_ID = "laboratorio-ai-460517"
DATASET_ID = "dataset"

# Display names mapping (DB name -> Human readable name)
TABLE_DISPLAY_NAMES = {
    "studenti": "Students Data",
    "studenti_churn_pred": "Dropout Prediction",
    "studenti_cluster": "Student Clustering",
    "studenti_soddisfazione_btr": "Satisfaction Analysis",
    "feature_importance_studenti": "Feature Importance",
    "report_finale_soddisfazione_studenti": "Satisfaction Report",
    "student_churn_rf": "Churn Model Details",
    "student_kmeans": "K-means Model Details"
}

TABLE_DESCRIPTIONS = {
    "studenti": "Student demographic and performance data",
    "studenti_churn_pred": "Dropout predictions with probability scores",
    "studenti_cluster": "Student segmentation via clustering",
    "studenti_soddisfazione_btr": "Student satisfaction analysis",
    "feature_importance_studenti": "Feature importance from predictive model",
    "report_finale_soddisfazione_studenti": "Complete satisfaction analysis report",
    "student_churn_rf": "Random Forest model details for dropout prediction",
    "student_kmeans": "K-means model details for behavioral clustering"
}

TABLE_ORIGINS = {
    "studenti": """**Origin:**

The `students` table collects demographic information and performance metrics for each student.
Source data comes from the university management system (student registry, grades, exams taken, etc.).
Before loading into BigQuery, a cleaning and normalization process was performed:
- Removal of duplicate records
- Standardization of date and string formats
- Calculation of new features (e.g., average grades, number of exams taken)
""",
    "studenti_churn_pred": """**Origin:**

This table contains dropout (churn) predictions generated by a **Random Forest** Machine Learning model.
**Main pipeline steps:**
1. Loading and cleaning base data from `students` and related tables.
2. Feature engineering: selection and transformation of the most relevant variables (e.g., average grades, study hours, event participation).
3. Splitting the dataset into training and test sets.
4. Training the Random Forest model (with parameter optimization via cross-validation).
5. Calculating churn probabilities for each student (`prob_churn` column) and predicted class (churn yes/no).
6. Saving results to this table, along with confidence level and predicted label.
""",
    "student_churn_rf": """**Origin:**

This table contains the details and metrics of the **Random Forest** model used to predict dropout.
Each row shows:
- A performance metric (e.g., accuracy, precision, recall) calculated on the test set.
- The optimal parameters used (number of trees, max depth, etc.).
Generated during the validation phase, after hyperparameter tuning and measuring performance on a hold-out set.
""",
    "feature_importance_studenti": """**Origin:**

This table shows the feature importance extracted from the `student_churn_rf` Random Forest model.
For each characteristic (`feature`) it includes:
- `importance_weight`: number of times the feature was selected for a split in the various trees of the model.
- `information_gain`: sum of information gained, indicating how much the feature contributed to reducing impurity.
- `coverage`: total number of examples in the dataset that passed through a node using that feature.
- `importance_percentage`: normalized weight on a [0,100] scale.
- `importance_category`: qualitative label (e.g., "Very Important", "Moderately Important", "Less Important").
This table is generated by taking the `feature_importances_` values from scikit-learn and saving them to BigQuery.
""",
    "studenti_cluster": """**Origin:**

The `student_cluster` table assigns each student to a cluster, obtained via the **K-means** algorithm.
**Main steps:**
1. Selection of significant numerical features (e.g., weekly study hours, average grades, number of absences).
2. Variable standardization (scaling) so they have mean = 0 and variance = 1.
3. Training K-means with K = 4 (number of clusters chosen via elbow method).
4. Calculation of the centroid for each cluster and assignment of the `cluster_id` label to each student.
5. Saving to this table of `cluster_id`, centroid coordinates, and distance of each student from their centroid.
""",
    "student_kmeans": """**Origin:**

This table contains the details of the **K-means (K = 4)** algorithm used for student clustering.
Includes:
- The coordinates of the centroids of each cluster.
- The inertia (sum of squared distances of points from their respective centroid) for each iteration (useful for verifying convergence).
Created during K-means training to analyze the quality of the subdivision.
""",
    "studenti_soddisfazione_btr": """**Origin:**

This table records the results of a **Boosted Tree** regression model (e.g., XGBoost) used to estimate student satisfaction levels.
**Main steps:**
1. Collection of satisfaction questionnaires (Likert scale 1-5).
2. Cleaning and recoding of responses (e.g., transformation into numerical values).
3. Creation of descriptive features (e.g., number of events attended, average grades, family income).
4. Training the Boosted Tree regression model to predict the satisfaction score.
5. Calculation of performance metrics (RÂ², RMSE) on a hold-out set.
6. Saving results to this table, with score estimates, confidence intervals, and most influential features.
""",
    "report_finale_soddisfazione_studenti": """**Origin:**

This report summarizes the student satisfaction analysis, based on the results of `student_satisfaction_btr`.
Includes:
- Distribution charts of satisfaction scores.
- Comparison between degree programs and student clusters.
- Operational suggestions to improve the student experience.
Generated automatically via a Python script that:
1. Creates various views in BigQuery.
2. Aggregates data into summary tables.
3. Produces a final PDF/HTML to share with the project team.
"""
}
